 - This is my class note about 2021 Fall Course "**Reinforcement Learning**" taught by Professor [Prashant Doshi](https://www.cs.uga.edu/directory/people/prashant-doshi)
 - Sutton, Richard S. and Andrew G. Barto. [**Reinforcement Learning: An Introduction.**](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) :star: :star: :star: :star: :star: 
 - [**Key Papers in Deep RL**](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) :star: :star: :star: :star: :star:  
 - Russell, Stuart J. and Peter Norvig. **Artificial Intelligence: A Modern Approach.**  

____________________________________________________
# Review of RL

**1. Introduction to RL**                    (Chapter 1 of S&B and Chapter 2 of R&N)                    
   - a. What is RL? Reward hypothesis and exploration-exploitation tradeoff :star:                   
   - b. Agents, environments, and types of environments :star:                     
   - c. Differentiate RL from others forms of ML      
   - d. Key Challenges in RL 
   - e. History of RL    

**2. Background on Probability Theory**        (Chapter 13 of R&N)     
   - a. Making decisions under uncertainty 
   - b. Probability basics and random variables 
   - c. Prior and conditional probability 
   - d. Bayes rule  

**3. Markov Decision Processes**:star:          (Chapters 3 and 4 of S&B and Chapter 17 (17.1-17.3) of R&N) 
   - a. MDP as a model for RL problems: definition :star: 
   - b. Bellman equation and Dynamic programming :star: 
   - c. Solution approaches: value iteration and policy iteration :star: 

**4. Model-based RL**                           (Chapter 8 (8.1-8.6) of S&B)    
   - a. Distribution and sample models 
   - b. Certainty Equivalence    
   - c. Sutton’s Dyna-Q algorithm and Prioritized Sweeping :star: 
   - d. Advantages and disadvantages for each method 

**5. Model-free RL: Value-based learning**:star:                             Chapters 5 (5.1-5.3), 6, 9, 12 (12.1-12.2) of S&B) 
   - a. On-policy methods
       - i. Monte-Carlo Control and Exploring Starts method :star:
       - ii. Temporal difference learning (TD prediction, TD learning)  :star:
       - iii. SARSA for control :star:    
       - iv. Bootstrapping and Sampling :star: 
       - v. Eligibility traces – TD(λ) :star:  
   - b. Off-policy methods 
       - i. Q-learning   :star:
       - ii. Maximization bias and double learning :star:
       - iii. Comparison between SARSA and Q-learning   :star:
       - iv. Function approximation and Deep Q-learning   :star:     

**6. Policy-gradient RL**:star:                             (Chapter 13 (13.1-13.4) of S&B)  
   - a. Benefit of approximating policies compared to values  :star: 
   - b. Parameterized policies   :star:  
   - c. Policy values, optimization, and policy gradients    :star:
   - d. Finite difference optimization  :star:   
   - e. Likelihood ratio policies and policy gradient theorem     :star:  
   - f. REINFORCE   :star:
  



